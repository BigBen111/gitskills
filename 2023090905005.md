# Q1
## 划分的板块： 
1. 监督学习
   算法： 线性回归、逻辑回归、决策树、支持向量机、神经网络。
   目的是让模型能够从已知的输入和输出之间的关系中学习，并且能够对新的输入做出正确的预测。
2. 无监督学习
   算法：聚类算法、降维算法，强化学习算法等
   没有明确的目的也不需要给数据打标签，对数据进行快速分类
3. 强化学习
   算法：时间差分学习，自适应动态规划等
   强化学习是智能体在和环境每一次交互中，根据当前的环境的情况，按照一定策略采取行动, 以获得回报；经过多次训练后获得最大化回报
   与有监督学习区别：
   |      | 有监督学习 | 强化学习 |
   | -----| ---- | ---- |
   | 样本顺序 | 无关 | 有关 |
   | 样本相关性 | 独立 | 相关 |
   |学习方式|批量学习|交互式|
4. 深度学习
   算法：卷积神经网络、循环神经网络、生成对抗网络等
   通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示
5. 迁移学习
   算法：（未知） 
   将从一个地方学到的规则进行跨领域的应用的学习方式
----
# Q2
## 机器学习应用领域
1. 自然语言处理：让计算机能够理解和处理自然语言
机器翻译、情感分析
2. 计算机视觉：让计算机能够理解和处理图像和视频
人脸识别、物体识别、行人跟踪、医学影像分析
3. 医疗诊断：利用机器学习技术对医疗图像和数据进行分析和处理
疾病诊断、药物研发、医疗影像分析、健康监测
4. 自动驾驶 
无人机、无人车等等
5. 人工智能游戏：利用机器学习技术实现智能游戏对战
围棋、扑克游戏、电子竞技等等
---
# Q3
链式法则：微积分求导的方法，用于求导复合函数。
作用：机器深度学习的目标之一就是让代价函数变小，链式法则能求各种函数的斜率，结合梯度下降的原理以便找出优化神经网络的参数设置方式。也用在执行反向传播（权值更新计算）上

泰勒公式：用函数某一点的信息描述函数附近值的公式，以便研究复杂函数性质。
作用：比如在XGBoost模型中用泰勒二阶展开来近似表示损失函数。梯度下降法中运用泰勒一阶展开最小化损失函数。

梯度：一个向量，是该点函数变化率取得的最大值的方向，模等于该变化率的值。便于合理更新参数让函数损失值减小的更快（机器学的更快）
***
# Q4
## 梯度下降
####朝着梯度的相反方向移动使得损失函数下降最快,本质上是利用方向导数的知识求得期望的方向。
由泰勒公式的一阶展开：
$$ f(x_{1})=f(x_{0})+(x_{1}-x_{0})\nabla f(x_{0})$$
得
$$ f(x_{1})-f(x_{0})=\nabla f(x_{0})$$
而
$$(x_{1}-x_{0})$$
是很小的一个向量
可理解为由$x_{0}$到$x_{1}$的向量v和模m的乘积
所以当
$$\nabla f(x)$$
和
$$(x_{1}-x_{0})$$
反向时得到最大下降梯度的方向
令
$$\frac {m}{\left\|\nabla f(x)\right\|}=步长$$
我们可以调节步长以调整下降效果和精度，常用的算法有BGD,SGD,mini-batch和Momentum等等。值得注意的是，选取多少个样本来计算梯度下降的方向是很重要的，选取太多数据导致计算速度慢；选取太少数据会使计算精度不够。
随机打乱数据，选取部分数据来计算是一个比较折中的方法。但是梯度下降可能会陷入局部最小值的问题，目前解决该问题的方法我还没有了解，但是若局部最小值满足我们的精读要求那也是可以接受的。

***关于方向导数和梯度的数学知识学习总结：***

方向导数是在函数定义域内对某一方向求得的该方向函数变化的趋势（某一方向的求导） **是一个数值** 
~~个人认为方向导数的定义不是那么重要~~ :smile:
梯度是一个函数在某一点处斜率最大值的方向 **是个矢量**

**求梯度先要会求偏导**
用定义
$$\Delta Z_{x}=lim \frac{f((x+\Delta x),y)-f(x,y)}{\Delta x}$$
或者固定x和y其中一个的值，当成一元函数求导都是不错的方法。（多元函数都类似）

得出各方向的偏导数之后
**求梯度**（以二元导数为例）：
$$gradf(x,y)=\left\{\frac{\delta f}{\delta x},\frac{\delta f}{\delta y}\right\}$$
代入偏导数的结果可得梯度方向
梯度方向的模是该函数各个元的平方和（类比平面向量的模长）

**方向导数的计算方法**
在给定的P点，已知导数的方向和x轴正方向夹角为$\beta$,则令射线L的原点在P点，方向为$(\cos \beta,\sin \beta)$. 利用向量点乘
$$\frac{\delta f}{\delta l}=\left\{\frac{\delta f}{\delta x},\frac{\delta f}{\delta y}\right\}·(\cos \beta,\sin \beta)$$
得出方向导数的值
注：*方向导数的值更像是梯度方向在方向导数所指方向的<u>投影长度</U>。当方向导数和梯度方向反向时（$\beta= \pi$)，就是我们要找的梯度下降最快的方向。*

:v:*学习多元函数和梯度的一些过程和想法：*
1.由入门教程推荐（3B1B）的三个科普视频开始对神经网络有了初步认识
2.在知乎B站上了解了梯度在机器学习的作用（大概的计算思路）和方向导数的数学概念
3.在学校慕课的微积分||重修上学习了多元函数的极限，连续性，求导方法，偏导数的概念和几何意义等。大致了解了例题的解决办法，认识了各种符号 $\delta$,$\frac{\delta f}{\delta x}$,$\nabla$的数学意义
__在没学多元微积分时，我认为它是一个很复杂很不友好的东西。但在机器学习的背景下，我觉得它并不难理解。逐渐感受到数学在工作中的意义
另外，markdown手输公式是真痛苦__:cry:
##反向传播
####在机器学习中，学习总是伴随着试错的过程，如果输出结果错的不可接受，就需要机器改变学习方法，若是模拟神经元的机器学习方式，此时一个比较好的调整方法就是通过反向传播，调整各节点的参数（神经元连接强度），使得结果趋于期望。

开始之前，我们要了解Sigmoid函数
$$S(X)=\frac{1}{1+e^{-x}}$$
是一个定义域从负无穷到正无穷，值域从0到1的S型单调递增函数。能有效把权重值压缩进0~1中。

==以下样例是在学习知乎B站教程后自己创造的情景(若有错误请指出)==:wink:

（图片在文件夹中）

###正向输入
输入层->隐含层：
$$inputM1=I1*W1+I2*W2=0.09$$
$$Sigmoid(0.09)=0.5224$$
同理：$$imputM2=0.115,Sigmoid(0.115)=0.5287$$

隐藏层->输出层：
$$O1=INPUTM1*W5+INPUTM2*W7=0.4213,Sigmoid(0.4213)=0.6037$$
同理$$Sigmoid(O2)=0.5847$$
$$E_{TOTAL}=0.5*\Sigma(target-output)^{2}=0.092$$
(数据没取好，$E_{total}$有点小)

###权值更新
(考虑到数据和计算量决定不带入数值了,重点在于链式求导法的使用):cry:
若我们想知道W6的值对$E_{TOTAL}$影响大小，需要对它求关于W6的导数。
经过分析:W6->O1->Sigmoid(O1)->$E_{1}$ 这个过程看做函数的层层嵌套。
求导：
$$\frac{\delta E_{TOTAL}}{\delta W6}=\frac{\delta E_{TOTAL}}{\delta E_{1}}*\frac{\delta E_{1}}{\delta Sigmoid(O1)}*\frac{\delta Sigmoid(O1)}{\delta O1}*\frac{\delta O1}{\delta W6}$$
*注：求导过程已经在草稿中尝试过，难度不大。*

更新权值（利用梯度公式）：
$$W6'=W6-N*\frac{\delta E_{TOTAL}}{\delta W6}$$
其中N为学习率
同理更新其他W值。

###隐含层的权值更新
从隐含层到驶入层的更新：比如求W2的更新值，需要注意:warning:有W5->M1->W2和W6->M1->W2两条线路，因此
$$\frac{\delta E_{TOTAL}}{\delta W2}=\frac{\delta E_{TOTAL}}{\delta M1}*\frac{\delta E_{TOTAL}}{\delta M2}$$

以上是反向传播的大致流程

:v:*反向传播注意事项和学习心得：*
1.反向传播机制并不是人脑神经元所用的机制，不是真正意义上的模仿神经网络，所以其解决的问题确实会有瓶颈。（大脑确实非常非常精密）
2.反向传播容易遇到局部最小值的问题
3.当权值接近0或1时，由于sigmoid函数原因，参数会很接近，导致学习速度受阻。
*很多科学家并不看好反向传播的发展，最近Hinton提出了Forward-Forward算法（了解中）
心得：
最初接触链式求导是在学习隐函数求导的时候。而链式求导其实在高中也经常使用，不过没有被规范化而已。可以看出，反向传播在基础层面的数学知识时很容易懂的，但在解决复杂问题时会遇到很多意想不到的困难。机器学习的理论一直在发展着，而深厚的数学基础在其中发挥着举足轻重的作用。

***
# Q5
##机器学习的本质
我认为，机器学习的各种算法都是在学习经验和模型，然后将经验运用到新的情景中（比如语音识别和文字识别）。这是人类在年幼阶段的主要学习方法之一。或是通过自我训练，自己创造数据以实现经验的快速积累。机器学习并没能模仿人演绎推理的过程，但是其强大的经验储备和计算能力足以替代某些中低层次的推理要求，比如下围棋，自动驾驶。这让人们一次又一次地思考什么才是我们独有的能力。真正要具有成年人创新创造和深度推理的能力，可能需要需要等人完全理解大脑构造（至少能模仿大脑结构）的时候才能实现。
在准备这道题的过程中，我也了解了阿法狗的基本原理，为人们能利用数学知识和计算机强大的计算优势，创造出比人更加“智慧”的系统而感到欣喜。但我逐步建立的数学基础，也为这神秘的背后添了一道理性的光彩。

***
##完成题目后的一些想法
做题之前，我没有料到这需要花一个星期的时间:cry:
做题的时候，总是有各种各样意想不到的问题蹦出来，从基本的markdown preview打不开，到学习一个新知识却各种术语看不懂，再到公式自己推导计算时发现其实还没有学会。
但无论怎么说，这道题给了我一个很好的学习指引，我对人工智能的了解开始进入数学和理论的层面。
（通过这个机会，我还发现git真的很好用）
机器学习注定是一个直面困难的过程