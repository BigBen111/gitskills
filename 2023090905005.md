# Q1
## 划分的板块： 
1. 监督学习
   算法： 线性回归、逻辑回归、决策树、支持向量机、神经网络。
   目的是让模型能够从已知的输入和输出之间的关系中学习，并且能够对新的输入做出正确的预测。
2. 无监督学习
   算法：聚类算法、降维算法，强化学习算法等
   没有明确的目的也不需要给数据打标签，对数据进行快速分类
3. 强化学习
   算法：时间差分学习，自适应动态规划等
   强化学习是智能体在和环境每一次交互中，根据当前的环境的情况，按照一定策略采取行动, 以获得回报；经过多次训练后获得最大化回报
   与有监督学习区别：
   |      | 有监督学习 | 强化学习 |
   | -----| ---- | ---- |
   | 样本顺序 | 无关 | 有关 |
   | 样本相关性 | 独立 | 相关 |
   |学习方式|批量学习|交互式|
4. 深度学习
   算法：卷积神经网络、循环神经网络、生成对抗网络等
   通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示
5. 迁移学习
   算法：（未知） 
   将从一个地方学到的规则进行跨领域的应用的学习方式
----
# Q2
## 机器学习应用领域
1. 自然语言处理：让计算机能够理解和处理自然语言
机器翻译、情感分析
2. 计算机视觉：让计算机能够理解和处理图像和视频
人脸识别、物体识别、行人跟踪、医学影像分析
3. 医疗诊断：利用机器学习技术对医疗图像和数据进行分析和处理
疾病诊断、药物研发、医疗影像分析、健康监测
4. 自动驾驶 
无人机、无人车等等
5. 人工智能游戏：利用机器学习技术实现智能游戏对战
围棋、扑克游戏、电子竞技等等
---
# Q3
链式法则：微积分求导的方法，用于求导复合函数。
作用：机器深度学习的目标之一就是让代价函数变小，链式法则能求各种函数的斜率，结合梯度下降的原理以便找出优化神经网络的参数设置方式。

泰勒公式：用函数某一点的信息描述函数附近值的公式，以便研究复杂函数性质。
作用：比如在XGBoost模型中用泰勒二阶展开来近似表示损失函数。梯度下降法中运用泰勒一阶展开最小化损失函数。

梯度：一个向量，是该点函数变化率取得的最大值的方向，模等于该变化率的值。便于合理更新参数让函数损失值减小的更快（机器学的更快）
***
# Q4
## 梯度下降
朝着梯度的相反方向移动使得损失函数下降最快,本质上是利用方向导数的知识求得期望的方向。
由泰勒公式的一阶展开：
$$ f(x_{1})=f(x_{0})+(x_{1}-x_{0})\Delta f(x_{0})$$
得
$$ f(x_{1})-f(x_{0})=\Delta f(x_{0})$$
而
$$x_{1}-x_{0}$$
是从$x_{0}到x_{1}$的向量v和模m的乘积
所以当
$$\Delta f(x)$$
和
$$x_{1}-x_{0}$$
反向时得到最大下降梯度的方向
令
$$\frac {m}{\left\|\Delta f(x)\right\|}=步长$$
我们可以调节步长以调整下降效果和精读，值得注意的是，选取多少个样本来计算梯度下降的方向是很重要的，选取太多数据导致计算速度慢；选取太少数据会使计算精度不够。随机打乱数据，选取部分数据来计算是一个比较折中的方法。但是梯度下降可能会陷入局部最小值的问题，目前解决该问题的方法我还没有了解，但是若局部最小值满足我们的精读要求那也是可以接受的。
##反向传播
